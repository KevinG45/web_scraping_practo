#!/usr/bin/env python3
"""
Run both Scrapy and Playwright scrapers for comprehensive doctor data collection
"""

import subprocess
import sys
import os
import json
from pathlib import Path

def run_scrapy_spider():
    """Run the Scrapy spider for initial URL discovery"""
    print("=" * 50)
    print("Phase 1: Running Scrapy spider for comprehensive URL discovery...")
    print("=" * 50)
    print("Starting Scrapy Spider...")
    print("=" * 50)
    
    try:
        # Change to the correct directory for scrapy
        os.chdir(os.path.dirname(os.path.abspath(__file__)))
        
        # Run scrapy spider
        result = subprocess.run([
            sys.executable, '-m', 'scrapy', 'crawl', 'doctor_spider',
            '-s', 'CLOSESPIDER_ITEMCOUNT=10',  # Limit for testing
            '-L', 'INFO'
        ], capture_output=True, text=True, timeout=300)
        
        print("Scrapy Output:")
        if result.stdout:
            print(result.stdout)
        
        print("\nScrapy Errors:")
        if result.stderr:
            print(result.stderr)
            
        return result.returncode == 0
        
    except subprocess.TimeoutExpired:
        print("‚ùå Scrapy spider timed out after 5 minutes")
        return False
    except Exception as e:
        print(f"‚ùå Error running Scrapy spider: {e}")
        return False

def run_playwright_scraper():
    """Run the Playwright scraper for detailed data extraction"""
    print("\n" + "=" * 50)
    print("Phase 2: Running Playwright scraper for detailed extraction...")
    print("=" * 50)
    
    try:
        # Import playwright modules
        from playwright.async_api import async_playwright
        import asyncio
        
        async def run_playwright():
            try:
                async with async_playwright() as p:
                    # Try to launch browser (will fail if not installed)
                    browser = await p.chromium.launch(headless=True)
                    context = await browser.new_context()
                    
                    # Import and initialize the scraper with proper arguments
                    from playwright_scraper.doctor_scraper import DoctorScraper
                    scraper = DoctorScraper(browser, context)
                    
                    # Use some test URLs for demonstration
                    test_urls = [
                        "https://www.practo.com/bangalore/doctor/dr-rajesh-kumar-cardiologist",
                        "https://www.practo.com/bangalore/doctor/dr-priya-sharma-dermatologist"
                    ]
                    
                    results = []
                    for url in test_urls:
                        print(f"Scraping: {url}")
                        try:
                            result = await scraper.scrape_doctor_page(url)
                            if result:
                                results.append(result)
                                print(f"‚úì Successfully scraped {result.get('name', 'Unknown')}")
                            else:
                                print(f"‚ùå Failed to scrape {url}")
                        except Exception as e:
                            print(f"‚ùå Error scraping {url}: {e}")
                    
                    await browser.close()
                    return results
            except Exception as e:
                if "Executable doesn't exist" in str(e) or "browser is not installed" in str(e):
                    print("‚ö†Ô∏è Playwright browser not installed. Run 'playwright install chromium' to enable Playwright scraping.")
                    return []
                else:
                    raise e
        
        # Run the async function
        results = asyncio.run(run_playwright())
        
        if results:
            print(f"\n‚úì Playwright scraper completed. Scraped {len(results)} profiles.")
            return True
        else:
            print("\n‚ö†Ô∏è Playwright scraper skipped (browser not available).")
            return True  # Still count as success since it's optional
        
    except Exception as e:
        print(f"‚ùå Error running Playwright scraper: {e}")
        return False

def main():
    """Main function to run both scrapers"""
    print("Practo Doctor Scraper - Running Both Scrapy and Playwright")
    print("=" * 60)
    
    # Run Scrapy spider first
    scrapy_success = run_scrapy_spider()
    
    # Run Playwright scraper
    playwright_success = run_playwright_scraper()
    
    # Summary
    print("\n" + "=" * 60)
    print("FINAL RESULTS:")
    print("=" * 60)
    print(f"Scrapy Spider: {'‚úì SUCCESS' if scrapy_success else '‚ùå FAILED'}")
    print(f"Playwright Scraper: {'‚úì SUCCESS' if playwright_success else '‚ùå FAILED'}")
    
    if scrapy_success and playwright_success:
        print("üéâ Both scrapers completed successfully!")
        return 0
    elif scrapy_success or playwright_success:
        print("‚ö†Ô∏è One scraper completed successfully, one failed")
        return 1
    else:
        print("‚ùå Both scrapers failed")
        return 2

if __name__ == "__main__":
    sys.exit(main())